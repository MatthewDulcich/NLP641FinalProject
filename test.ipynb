{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "\n",
    "# Path to the directory containing train2014 images\n",
    "image_dir = os.path.join(parent_dir, \"NLP641FinalProject\", \"COCO_dataset\", \"train2014\")\n",
    "\n",
    "# Path to the captions mapping JSON file\n",
    "json_file = os.path.join(parent_dir, \"NLP641FinalProject\", \"COCO_dataset\", \"captions_mapping_train.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file, \"r\") as f:\n",
    "    captions_mapping = json.load(f)\n",
    "\n",
    "image_files = os.listdir(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(captions_mapping), len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_file in image_files:\n",
    "#     image_path = 'COCO_dataset/val2014/' + image_file\n",
    "#     if image_path not in captions_mapping:\n",
    "#         os.remove(image_path)\n",
    "#         print(f\"Image {image_path} deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(captions_mapping), len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "# Load the base model\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Preprocess the image\n",
    "image_path = 'dog.jpeg'\n",
    "img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n",
    "img_tensor = tf.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# Pass the image tensor through the model\n",
    "cnn_model = tf.keras.Model(inputs=base_model.input, outputs=base_model.output)\n",
    "output = cnn_model(img_tensor)\n",
    "\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Set Metal as the default compute device\n",
    "# tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "\n",
    "# Load ResNet50 without top layers\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',  # Or specify None if you don't want to use pre-trained weights\n",
    "    input_shape=(224, 224, 3)  # Adjust input shape according to your needs\n",
    ")\n",
    "resnet50.trainable = False  # Freeze the weights of the ResNet50 base model\n",
    "\n",
    "# Define additional layers\n",
    "output = resnet50.output\n",
    "output = tf.keras.layers.Reshape((-1, 2048))(output)  # Reshape the output to match the desired shape\n",
    "output = tf.keras.layers.Dense(1280)(output)  # Add a dense layer to reduce dimensionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "cnn_model = tf.keras.Model(inputs=resnet50.input, outputs=output)\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = 'dog.jpeg'\n",
    "img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.keras.applications.resnet50.preprocess_input(img_array)  # Preprocess input for ResNet50\n",
    "\n",
    "# Process the image through the model\n",
    "features = cnn_model.predict(tf.expand_dims(img_array, axis=0))\n",
    "\n",
    "print(features.shape)  # Check the shape of the extracted features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/_3xy56md31d24tz9rgd5r83w0000gn/T/ipykernel_75717/3198675487.py:5: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 1280)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Load the base model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Preprocess the image\n",
    "image_path = 'dog.jpeg'\n",
    "img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "img_tensor = tf.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# Pass the image tensor through the model\n",
    "cnn_model = tf.keras.Model(inputs=base_model.input, outputs=base_model.output)\n",
    "output = cnn_model(img_tensor)\n",
    "\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 12:53:42.012707: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-04-24 12:53:42.013009: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-24 12:53:42.013021: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-04-24 12:53:43.125882: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-04-24 12:53:43.126189: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-24 12:53:43.126200: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:coremltools:When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_targer' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://coremltools.readme.io/docs/unified-conversion-api#target-conversion-formats\n",
      "2024-04-24 12:53:45.262930: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-04-24 12:53:45.263326: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-24 12:53:45.263338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-04-24 12:53:46.631111: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-04-24 12:53:46.632812: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-24 12:53:46.632830: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Running TensorFlow Graph Passes:   0%|          | 0/6 [00:00<?, ? passes/s]2024-04-24 12:53:47.269046: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-24 12:53:47.269065: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 24.93 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 297/297 [00:00<00:00, 4909.81 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 1375.50 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 71/71 [00:00<00:00, 117.73 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1171.54 passes/s]\n",
      "/Users/thilakcm/anaconda3/envs/ML-AI/lib/python3.11/site-packages/coremltools/converters/mil/backend/mil/load.py:581: UserWarning: Some dimensions in the input shape are unknown, hence they are set to flexible ranges with lower bound and default value = 1, and upper bound = 2. To set different values for the default shape and upper bound, please use the ct.RangeDim() method as described here: https://coremltools.readme.io/docs/flexible-inputs#set-the-range-for-each-dimension.\n",
      "  warnings.warn(\n",
      "/Users/thilakcm/anaconda3/envs/ML-AI/lib/python3.11/site-packages/coremltools/converters/mil/backend/mil/load.py:591: UserWarning: There is \"None\" dim in TF input placeholder. Please consider specifying input shapes by using the \"inputs\" param in ct.convert().\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "saved_model_path = 'saved_model/'\n",
    "tf.saved_model.save(cnn_model, saved_model_path)\n",
    "\n",
    "# Convert the TensorFlow SavedModel to CoreML\n",
    "model = ct.convert(saved_model_path)\n",
    "\n",
    "model.save('converted_model.mlpackage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Provided key \"input_image\", in the input dict, does not match any of the model input name(s), which are: inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Perform inference and measure time\u001b[39;00m\n\u001b[1;32m     15\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m inference_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Print inference result\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-AI/lib/python3.11/site-packages/coremltools/models/model.py:596\u001b[0m, in \u001b[0;36mMLModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    593\u001b[0m MLModel\u001b[38;5;241m.\u001b[39m_check_predict_data(data)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__proxy__:\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMLModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__proxy__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_and_convert_input_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m# Error case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _macos_version() \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m13\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-AI/lib/python3.11/site-packages/coremltools/models/model.py:647\u001b[0m, in \u001b[0;36mMLModel._get_predictions\u001b[0;34m(proxy, preprocess_method, data)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_predictions\u001b[39m(proxy, preprocess_method, data):\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 647\u001b[0m         \u001b[43mpreprocess_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mpredict(data)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-AI/lib/python3.11/site-packages/coremltools/models/model.py:584\u001b[0m, in \u001b[0;36mMLModel.predict.<locals>.verify_and_convert_input_dict\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_and_convert_input_dict\u001b[39m(d):\n\u001b[0;32m--> 584\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_input_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_tensor_to_numpy(d)\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m# TODO: remove the following call when this is fixed: rdar://92239209\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-AI/lib/python3.11/site-packages/coremltools/models/model.py:716\u001b[0m, in \u001b[0;36mMLModel._verify_input_dict\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_verify_input_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dict):\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# Check if the input name given by the user is valid.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# Although this is checked during prediction inside CoreML Framework,\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# we still check it here to return early and\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;66;03m# return a more verbose error message\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_input_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;66;03m# verify that the pillow image modes are correct, for image inputs\u001b[39;00m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_pil_image_modes(input_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-AI/lib/python3.11/site-packages/coremltools/models/model.py:751\u001b[0m, in \u001b[0;36mMLModel._verify_input_name_exists\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m given_input \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_input_names_set:\n\u001b[1;32m    749\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided key \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, in the input dict, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    750\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match any of the model input name(s), which are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 751\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(err_msg\u001b[38;5;241m.\u001b[39mformat(given_input, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(model_input_names)))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Provided key \"input_image\", in the input dict, does not match any of the model input name(s), which are: inputs'"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Load the CoreML model\n",
    "model = ct.models.MLModel('converted_model.mlpackage')\n",
    "\n",
    "# Prepare input data (replace 'input_image.jpg' with your test image)\n",
    "image = Image.open('cat.jpeg').resize((224, 224))\n",
    "image = np.array(image) / 127.0 - 1.0\n",
    "input_data = {'input_image': image}\n",
    "\n",
    "# Perform inference and measure time\n",
    "start_time = time.time()\n",
    "output = model.predict(input_data)\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# Print inference result\n",
    "print(\"Inference result:\", output)\n",
    "\n",
    "# Print inference time\n",
    "print(\"Inference time:\", inference_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "cifar = tf.keras.datasets.cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(32, 32, 3),\n",
    "    classes=100,)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
